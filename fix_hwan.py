# app.py

import os
import streamlit as st
import pandas as pd
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain_anthropic import ChatAnthropicMessages

# âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
if not os.getenv("ANTHROPIC_API_KEY"):
    st.error("âŒ .env íŒŒì¼ì— ANTHROPIC_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
    st.stop()

# âœ… ìƒìˆ˜ ì„¤ì •
ROOT_DIR = "C:/KDT13/kh0616/project_12/hyundaicar_info"
PERSIST_DIR = "vector_store_index"

# âœ… ì „ì²´ PDF ìë™ ë¡œë”© â†’ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± í•¨ìˆ˜
def load_all_car_pdfs_to_vectorstore(root_path: str, persist_dir: str = "vector_store_index") -> Chroma:
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    all_docs = []

    for category in os.listdir(root_path):
        category_path = os.path.join(root_path, category)
        if not os.path.isdir(category_path):
            continue

    # ì°¨ëŸ‰ ëª¨ë¸ë³„ í´ë” íƒìƒ‰
        for car_model in os.listdir(category_path):
            car_model_path = os.path.join(category_path, car_model)
            if not os.path.isdir(car_model_path):
                continue

            for filename in os.listdir(car_model_path):
                if filename.endswith(".pdf"):
                    pdf_path = os.path.join(car_model_path, filename)
                    try:
                        docs = PyPDFLoader(pdf_path).load()
                        chunks = splitter.split_documents(docs)
                        all_docs.extend(chunks)
                        print(f"âœ… ë¡œë”© ì™„ë£Œ: {pdf_path}")
                    except Exception as e:
                        print(f"âŒ ë¡œë”© ì‹¤íŒ¨: {pdf_path} ({e})")

    st.success(f"ğŸ“¦ ì´ ë¬¸ì„œ ìˆ˜: {len(all_docs)}")
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vectordb = Chroma.from_documents(all_docs, embedding=embeddings, persist_directory=persist_dir)
    return vectordb

# âœ… Streamlit UI ì‹œì‘
st.set_page_config(page_title="ğŸš— í˜„ëŒ€ì°¨ Claude RAG ë°ëª¨", layout="wide")
st.title("ğŸš— í˜„ëŒ€ì°¨ Claude ê¸°ë°˜ RAG ë°ëª¨")
st.caption("í´ë”ì— ì €ì¥ëœ PDF ë¬¸ì„œë¥¼ ìë™ìœ¼ë¡œ ì½ì–´ Claude ê¸°ë°˜ ë‹µë³€ ìƒì„±")

# âœ… ìºì‹œëœ ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
@st.cache_resource
def get_vectordb():
    return load_all_car_pdfs_to_vectorstore(ROOT_DIR, PERSIST_DIR)

vectordb = get_vectordb()

# âœ… ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
question = st.text_input("â“ í˜„ëŒ€ì°¨ ì°¨ëŸ‰ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 'ìŠ¤íƒ€ë¦¬ì•„ì˜ ì ì¬ ìš©ëŸ‰ì€?')")

# âœ… ì§ˆë¬¸ ì²˜ë¦¬
if question:
    with st.spinner("ğŸ” Claude ê¸°ë°˜ ì‘ë‹µ ìƒì„± ì¤‘..."):
        retriever = vectordb.as_retriever()
        llm = ChatAnthropicMessages(
            model="claude-3-5-sonnet-20240620",
            temperature=0,
            api_key=os.getenv("ANTHROPIC_API_KEY")
        )
        qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)
        response = qa_chain.run(question)

    # âœ… ê²°ê³¼ ì¶œë ¥
    st.markdown("### ğŸ’¡ Claude ê¸°ë°˜ PDF ì‘ë‹µ")
    st.write(response)
